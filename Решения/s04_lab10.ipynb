{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f9a794",
   "metadata": {},
   "source": [
    "# Лабораторная 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd921c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d90563",
   "metadata": {},
   "source": [
    "## 1. Постановка задачи обучения с учителем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f2f11",
   "metadata": {},
   "source": [
    "* $X$ - множество объектов, или же, что более точно множество их информационных описаний\n",
    "* $Y$ - множество ответов \n",
    "* $y: X \\rightarrow Y$ - некоторая неизвестная зависимость\n",
    "\n",
    "***Дано:*** \n",
    "\n",
    "$ {x_1 ... x_l} \\subset X $ - объекты\n",
    "\n",
    "$y_1 = y(x_1) ... y_l = y(x_l)$ - ответы\n",
    "\n",
    "***Требуется найти:*** \n",
    "\n",
    "$a: X \\rightarrow Y $ - алгоритм, приближающий $y$ на множестве $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38269c48",
   "metadata": {},
   "source": [
    "## 2. Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac8faa",
   "metadata": {},
   "source": [
    "1. ***Среднеквадратичная ошибка (Mean Squared Error - MSE)*** для задачи регрессии:\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "где $y_i$ - фактическое значение, $\\hat{y_i}$ - предсказанное значение, и $n$ - количество объектов в выборке.\n",
    "\n",
    "2. ***Средняя абсолютная ошибка (Mean Absolute Error - MAE)*** для задачи регрессии:\n",
    "$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}| $$\n",
    "\n",
    "3. ***Кросс-энтропия (Cross-Entropy Loss) для задачи классификации*** в бинарном случае:\n",
    "$$\\text{Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}))$$\n",
    "где $y_i$ - фактическое бинарное значение (0 или 1), а $\\hat{y_i}$ - предсказанная вероятность принадлежности к классу 1.\n",
    "\n",
    "4. ***Кросс-энтропия (Cross-Entropy Loss) для задачи классификации*** в многомерном случае:\n",
    "\n",
    "$$\\text{Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{ij} \\log(p_{ij})$$\n",
    "\n",
    "где:\n",
    "\n",
    "$N$ - количество примеров в наборе данных\n",
    "\n",
    "$M$ - количество классов\n",
    "\n",
    "$y_{ij}$ - бинарное значение (0 или 1), которое указывает, принадлежит ли (i)-ый пример к (j)-ому классу\n",
    "\n",
    "$p_{ij}$ - предсказанная моделью вероятность того, что (i)-ый пример принадлежит к (j)-ому классу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7b8ab",
   "metadata": {},
   "source": [
    "## 3.1 Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae92342",
   "metadata": {},
   "source": [
    "Предположим, у нас есть функция потерь $L(w)$, где $w$ - это вектор параметров, который мы хотим оптимизировать. Наша цель - минимизировать эту функцию потерь путем изменения параметров $w$. Градиентный спуск осуществляет это изменение, используя градиент функции потерь.\n",
    "\n",
    "Шаги градиентного спуска выглядят следующим образом:\n",
    "\n",
    "1. Инициализируем параметры $w$ случайными значениями или нулями.\n",
    "2. Для каждой итерации $t$ обновляем параметры по формуле:\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\nabla L(w^{(t)})$$\n",
    "Где $\\alpha$ - это ***коэффициент обучения (learning rate)***, который определяет размер шага, а $\\nabla L(w^{(t)})$ - градиент функции потерь по параметрам $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e3088",
   "metadata": {},
   "source": [
    "## 3.2 Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f9a02",
   "metadata": {},
   "source": [
    "Математический процесс стохастического градиентного спуска заключается в следующих шагах:\n",
    "\n",
    "1. Выбирается случайный пример(или примеры) из обучающего набора с меткой ($x^{(i)}, y^{(i)})$.\n",
    "2. Вычисляется градиент функции потерь по параметрам модели для этого примера: $\\nabla L^{(i)}(w)$.\n",
    "3. Обновляются параметры модели $w$ в направлении, противоположном градиенту, с использованием скорости обучения $\\alpha$: $w := w - \\alpha \\nabla L^{(i)}(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf878fe",
   "metadata": {},
   "source": [
    "## 3.3 Оптимизатор Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115b94c",
   "metadata": {},
   "source": [
    "***Оптимизатор Adam (Adaptive Moment Estimation)*** - это популярный алгоритм оптимизации для обновления весов нейронной сети в процессе обучения. Adam является комбинацией двух других алгоритмов оптимизации - метода адаптивной скорости обучения (AdaGrad) и метода стохастического градиентного спуска (SGD).\n",
    "\n",
    "С математической точки зрения, обновление весов в Adam происходит с учетом оценки первого и второго моментов градиента. Для каждого параметра вычисляются средние моменты градиента (mean) и квадраты градиента (variance). Затем происходит корректировка этих моментов для учета bias смещения и сглаживания.\n",
    "\n",
    "Формула обновления весов в оптимизаторе Adam выглядит следующим образом: $$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n",
    "\n",
    "$$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 $$\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $$\n",
    "\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$ \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t $$\n",
    "\n",
    "Где:\n",
    "\n",
    "* $m_t$ - оценка первого момента градиента\n",
    "* $v_t$ - оценка второго момента градиента\n",
    "* $\\beta_1$ и $\\beta_2$ - коэффициенты сглаживания\n",
    "* $\\alpha$ - скорость обучения\n",
    "* $\\epsilon$ - маленькое число для стабильности\n",
    "* $\\hat{m}_t$ и $\\hat{v}_t$ - скорректированные оценки моментов\n",
    "* $\\theta_t$ и $\\theta_{t+1}$ - веса на предыдущем и текущем шаге, соответственно\n",
    "* $g_t$ - градиент функции потерь по весам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8725a33",
   "metadata": {},
   "source": [
    "## 4. Алгоритмы и модели обучения с учителем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15419390",
   "metadata": {},
   "source": [
    "Нами были рассмотрены следующие алгоритмы обучения с учителем:\n",
    "\n",
    "1. Метод ближайщих соседей. \n",
    "2. Линейная классификация и регрессия. \n",
    "3. Метод опорных векторов(SVM). \n",
    "4. Решающие дерево. \n",
    "5. Случайный лес. \n",
    "6. Градиентный бустинг(Catboost).\n",
    "7. Полносвязная нейронная сеть. \n",
    "8. Свёрточная нейронная сеть. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8049cd9",
   "metadata": {},
   "source": [
    "## 4.1 Метод ближайщих соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b856a5",
   "metadata": {},
   "source": [
    "***Основные гиперпараметры этого классификатора***:\n",
    "\n",
    "1. `n_neighbors`: Число соседей, которые используются для классификации каждого образца. Это, вероятно, самый важный гиперпараметр в классификаторе k-NN.\n",
    "\n",
    "2. `weights`: Этот параметр позволяет задать веса, используемые для голосования среди соседей. Возможные значения:\n",
    "- `uniform`: все соседи вносят одинаковый вклад в голосование.\n",
    "- `distance`: вес каждого соседа зависит от расстояния, то есть ближайшие соседи имеют больший вес.\n",
    "\n",
    "3. `p`: Параметр метрики расстояния. По умолчанию, p=2, что соответствует евклидовой метрике. Однако можно установить p=1, чтобы использовать манхэттенскую метрику.\n",
    "\n",
    "4. `metric`: Метрика расстояния. По умолчанию используется метрика Минковского, которая имеет параметр p. Однако можно также указать другие метрики, такие как косинусное расстояние, Жаккарда и другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f617d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект классификатора k-ближайших соседей\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# Обучаем модель на обучающих данных\n",
    "knn.fit(X_train, y_train)\n",
    "# Делаем предсказания на тестовых данных\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29cab0",
   "metadata": {},
   "source": [
    "## 4.2 Линейная классификация и регрессия \n",
    "\n",
    "***L1 и L2 регуляризация*** - это методы добавления штрафа за сложность модели во время обучения, который помогает предотвратить переобучение. Регуляризация добавляет дополнительный член в функцию потерь модели, который зависит от весов (параметров) модели, тем самым штрафуя большие веса и предотвращая их излишнюю сложность.\n",
    "\n",
    "1. ***L1 Регуляризация***:\n",
    "Также известная как Lasso регуляризация. В L1 регуляризации дополнительный член в функции потерь представляет собой сумму абсолютных значений весов модели:\n",
    "$L1: \\lambda \\sum_{i=1}^{n} |w_i|$\n",
    "где $( \\lambda $) - коэффициент регуляризации, который контролирует величину штрафа, а $( w_i $) - веса модели.\n",
    "\n",
    "Преимуществом L1 регуляризации является ее способность к отбору признаков - некоторые веса могут быть точно нулевыми, что делает ее полезной при работе с большим количеством признаков, отбирая наиболее важные из них.\n",
    "\n",
    "2. ***L2 Регуляризация***:\n",
    "Также известная как Ridge регуляризация. В L2 регуляризации дополнительный член в функции потерь представляет собой сумму квадратов весов модели:\n",
    "$L2: \\lambda \\sum_{i=1}^{n} w_i^2$\n",
    "где $( \\lambda $) - коэффициент регуляризации, а $( w_i $) - веса модели.\n",
    "\n",
    "L2 регуляризация штрафует большие веса, но не склонна делать их точно нулевыми. Это делает ее полезной при предотвращении переобучения и улучшении обобщающей способности модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122309e",
   "metadata": {},
   "source": [
    "## 4.3 SVM \n",
    "\n",
    "***Основные гиперпараметры:***\n",
    "\n",
    "1. ***C (Regularization Parameter)***: Гиперпараметр C контролирует баланс между достижением минимальной ошибки классификации на тренировочном наборе данных и минимизацией штрафа за нарушение мягкой границы между классами. Большее значение C приводит к более жесткой границе решения, увеличивая штраф за классификационные ошибки, тогда как меньшее значение C позволяет некоторым точкам нарушать границу.\n",
    "\n",
    "2. ***Kernel Type***: SVM поддерживает различные типы ядер, такие как линейное, полиномиальное и радиальное (RBF). Наиболее часто используемыми являются радиальное и полиномиальное ядра.\n",
    "\n",
    "3. ***Gamma (Kernel Coefficient)***: Гиперпараметр gamma сочетает важность точек обучающего набора при вычислении разделяющей поверхности. Большие значения gamma могут привести к переобучению, в то время как маленькие значения могут привести к недообучению.\n",
    "\n",
    "4. ***Degree (Polynomial Kernel)***: Если вы используете полиномиальное ядро, гиперпараметр степени определяет степень полинома.\n",
    "\n",
    "5. ***Class Weights***: Этот параметр позволяет учитывать несбалансированные классы в данных путем назначения разных весов классам. Это может помочь улучшить производительность модели в случае несбалансированных данных.\n",
    "\n",
    "6. ***Convergence Criteria***: Этот параметр определяет критерии остановки обучения модели, такие как минимальное значение функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a808af",
   "metadata": {},
   "source": [
    "## 4.4 Решающее дерево\n",
    "\n",
    "***Гиперпараметры:***\n",
    "\n",
    "***criterion*** = 'gini' - функция для измерения качества разбиения. Поддерживаются критерии 'gini' для неодородности Джини и 'entropy' для прироста информации\n",
    "\n",
    "***splitter***='best' - стратегия, используемая для выбора разбиения в каждом узле. Поддерживаются стратегии 'best' для выбора лучшего разбиения и 'random' для выбора лучшего рандомного разбиения\n",
    "\n",
    "***max_depth***=None - максимальная глубина дерева. Если None, то дерево строится до тех пор, пока в каждом листе не будет только один класс или пока каждый лист не будет содержать количество экземпляров, равное min_samples_split.\n",
    "\n",
    "***max_leaf_nodes***=None - максимальное количество листьев. Дерево строится исходя из ограничения на максимальное количество листьев. Остаются только те листья, которые максимально уменьшают неоднородность.\n",
    "\n",
    "***min_impurity_decrease***=0.0 - минимальное уменьшение неоднородности. Узел расщепляется, если неоднородность уменьшается на число больше или равное min_impurity_decrease.\n",
    "\n",
    "Это лишь самые популярные гиперпараметры. На самом деле их намного больше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение модели решающего дерева\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea3b79",
   "metadata": {},
   "source": [
    "## 4.5 Случайный лес \n",
    "\n",
    "***Гиперпараметры:***\n",
    "\n",
    "***n_estimators***: количество деревьев в случайном лесу.\n",
    "\n",
    "***criterion***: функция для измерения качества разделения. Для классификации обычно используется \"gini\", а для регрессии - \"mse\" (среднеквадратичная ошибка).\n",
    "\n",
    "***max_depth***: максимальная глубина каждого дерева.\n",
    "\n",
    "***min_samples_split***: минимальное количество образцов, необходимое для разделения внутреннего узла.\n",
    "\n",
    "***min_samples_leaf***: минимальное количество образцов в листовом узле.\n",
    "\n",
    "***max_features***: количество признаков, рассматриваемых при разделении.\n",
    "\n",
    "Это лишь самые популярные гиперпараметры. На самом деле их намного больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение модели случайного леса\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "# Прогнозирование на тестовом наборе\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e8b92",
   "metadata": {},
   "source": [
    "## 4.6 Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157d369",
   "metadata": {},
   "source": [
    "***Catboostclassifier - гиперпараметры:***\n",
    "\n",
    "__learning_rate__: Скорость обучения (learning rate). Этот параметр контролирует величину изменения весов на каждом шаге градиентного спуска, и влияет на скорость сходимости модели.\n",
    "\n",
    "__depth__: Максимальная глубина деревьев. Определяет максимальную глубину деревьев, которые используются в градиентном бустинге.\n",
    "\n",
    "__l2_leaf_reg__: L2 регуляризация. Этот параметр контролирует силу регуляризации, помогая предотвратить переобучение модели.\n",
    "\n",
    "__iterations__: Количество итераций (деревьев), которые будут созданы в градиентном бустинге.\n",
    "\n",
    "__border_count__: Параметр, определяющий, сколько значений должно быть у категориального признака, чтобы считать его категориальным. Это позволяет модели эффективно обрабатывать категориальные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e799ab",
   "metadata": {},
   "source": [
    "## 4.7 Полносвязная нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a89fe",
   "metadata": {},
   "source": [
    "***Модель нейрона*** - это математическая модель. Она обычно состоит из следующих элементов:\n",
    "\n",
    "1. **Входные данные (Input):** На вход модели нейрона поступают признаки или значения, которые необходимо обработать. На картинке выше - это $x_1, x_2, ... x_N$.\n",
    "\n",
    "2. **Веса (Weights):** Каждый входной признак соотносится с определенным весом, который отражает его важность для вычислений. В процессе обучения модель обучается и веса меняются таким образом, чтобы уменьшить функцию потерь. На картинке выше - это $w_0, w_1, w_2, ... w_N$.\n",
    "\n",
    "3. **Сумматор (Aggregator):** В сумматоре происходит вычисление скалярного произведения $u = <x,w> = w_0 \\cdot 1 + w_1 \\cdot x_1 + ... w_N \\cdot x_N$. \n",
    "\n",
    "4. **Функция активации (Activation Function):** Результат сумматора $u$ проходит через функцию активации $f(u)$, которая добавляет нелинейность в модель. \n",
    "\n",
    "5. **Выход (Output):** На основе результата функции активации нейрон генерирует свое значение - $y$, которое затем может передаваться другим нейронам. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e0c24",
   "metadata": {},
   "source": [
    "Рассмотрим пару моментов, которые мы не рассматривали подробно до этого:\n",
    "\n",
    "***Затухание градиента (vanishing gradient)*** - это проблема, возникающая в глубоких нейронных сетях во время обратного распространения ошибки. Эта проблема заключается в том, что градиент ошибки, вычисляемый на более глубоких уровнях нейронной сети, становится очень маленьким по сравнению с начальными слоями, что затрудняет обновление их весов.\n",
    "\n",
    "Затухание градиента возникает из-за использования активационных функций, таких как ***сигмоида или гиперболический тангенс***, которые имеют ограниченный диапазон значений и производных. При обратном распространении ошибки градиент передается через слои с активационными функциями, и если этот градиент становится очень маленьким, то веса на этих слоях не обновляются должным образом.\n",
    "\n",
    "Для решения проблемы затухания градиента были разработаны различные подходы, такие как использование активационных функций с более широким диапазоном значений (например, ***ReLU***), ***методы инициализации весов***, ***батч-нормализация*** и различные методы оптимизации, такие как RMSprop, ***Adam*** и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c88dd",
   "metadata": {},
   "source": [
    "***Паралич сети (англ. \"network paralysis\")*** - это состояние, когда нейронная сеть перестает обучаться или производить правильные результаты из-за различных проблем. Паралич сети может возникнуть по разным причинам, например:\n",
    "\n",
    "1. ***Недостаточное количество обучающих данных***: если нейронная сеть не обучается на достаточном объеме данных, она может не смочь уловить все закономерности и производить неточные результаты.\n",
    "\n",
    "2. ***Неверно подобранные гиперпараметры***: нейронные сети имеют множество гиперпараметров (например, скорость обучения, количество эпох обучения и т.д.), и если они выбраны неправильно, это может привести к параличу сети.\n",
    "\n",
    "3. ***Проблемы с градиентным спуском***: градиентный спуск - это алгоритм оптимизации, используемый для обновления весов нейронной сети в процессе обучения. Если возникают проблемы с градиентом (например, исчезающий/взрывающийся градиент), это может вызвать паралич сети.\n",
    "\n",
    "4. ***Переобучение***: когда модель слишком хорошо запоминает обучающие данные и не обобщает свои знания на новые данные, это также может привести к параличу сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b3457",
   "metadata": {},
   "source": [
    "## 4.8 Свёрточная нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3b6f1",
   "metadata": {},
   "source": [
    "Данную модель мы подробно рассматривали на прошлом семинаре. Основными элементами являются:\n",
    "\n",
    "* ***Свёртка***\n",
    "* ***Pooling***\n",
    "* ***Dropout***\n",
    "* ***батч-нормализация***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d25653",
   "metadata": {},
   "source": [
    "Вместо построение своей архитектуры мы можем использовать готовые модели. ***См код ниже***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd8fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a1c06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14756\\1044681558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             )\n\u001b[1;32m--> 522\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Загрузка и предобработка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Ресайз изображений до 224x224 (так как AlexNet принимает изображения такого размера)\n",
    "    transforms.ToTensor(),  # Преобразование в тензор\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Нормализация данных\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Создание модели AlexNet\n",
    "model = models.alexnet().to(device)  # Загрузка модели AlexNet c предварительно обученными весами\n",
    "model.classifier[6] = nn.Linear(4096, 10)  # Замена последнего полносвязного слоя для соответствия 10 классам в CIFAR-10\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(5):  # 5 эпох обучения\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs.to(device), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # выводим статистику каждых 1000 мини-пакетов\n",
    "            print(f'Эпоха {epoch + 1}, Пакет {i + 1}, Потери: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Обучение завершено')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19aaf1b",
   "metadata": {},
   "source": [
    "## 5. Метрики оценки качества модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6e225",
   "metadata": {},
   "source": [
    "1. **Точность (Accuracy)**: \n",
    "\n",
    "2. **Полнота (Recall)**: Полнота измеряет, какую долю объектов положительного класса модель способна обнаружить. Она особенно важна в задачах, где ложноотрицательные ошибки критичны.\n",
    "\n",
    "3. **Точность (Precision)**: Показывает, какую долю из предсказанных положительных примеров реально являются положительными. Precision вычисляется как отношение истинно положительных результатов к сумме истинно положительных и ложноположительных результатов.\n",
    "\n",
    "4. **F1-мера (F1-score)**: Комбинирует в себе полноту и точность в одну метрику и вычисляется как гармоническое среднее между полнотой и точностью. F1-мера хорошо работает в случаях, когда нужно найти баланс между полнотой и точностью.\n",
    "\n",
    "5. **ROC AUC**: Эта метрика используется для оценки качества бинарной классификации и представляет собой площадь под кривой ROC (Receiver Operating Characteristic). \n",
    "\n",
    "6. **MSE (Mean Squared Error)**: Это метрика для оценки качества регрессионных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ffb682",
   "metadata": {},
   "source": [
    "## 6. Обучение без учителя"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e5948",
   "metadata": {},
   "source": [
    "***Дано:***\n",
    "\n",
    "$X$ - пространство объектов\n",
    "\n",
    "$X^l$ - обучающая выборка\n",
    "\n",
    "$\\rho: X * X -> [0;\\infty)$ - функция расстояния между объектами\n",
    "\n",
    "***Найти:***\n",
    "\n",
    "$Y$ - множество кластеров\n",
    "\n",
    "$a : X -> Y$ - алгоритм кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ba810",
   "metadata": {},
   "source": [
    "Существует несколько методов решения задачи кластеризации:\n",
    "1. **K-средних (K-means)**: Этот метод разбивает данные на заранее заданное количество кластеров (K) и минимизирует среднее расстояние между объектами и их центрами в каждом кластере.\n",
    "2. **Иерархическая кластеризация**: Этот метод строит иерархию кластеров, начиная с того, что каждый объект начинает как отдельный кластер, и затем объединяет их постепенно в более крупные кластеры.\n",
    "3. **DBSCAN**: Этот метод основан на плотности данных и способен обнаруживать кластеры произвольной формы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a6c32",
   "metadata": {},
   "source": [
    "## 7. Схема обучения моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29270464",
   "metadata": {},
   "source": [
    "1. Сбор датасета(это может быть довольно трудоёмкий процесс).\n",
    "2. Подготовка набора данных(в формате csv или в виде тензоров, удаление пропусков, удаление признаков и т.д).\n",
    "3. Выбор модели ML.\n",
    "4. Выбор гиперпараметров модели ML. \n",
    "5. Оценка полученных результатов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e5275",
   "metadata": {},
   "source": [
    "Выбор модели неплохо описан здесь: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea74de7",
   "metadata": {},
   "source": [
    "Данная схема не учитывает нейронные сети. \n",
    "\n",
    "Стоит понимать, что в случае табличных данных обычно лучшую точность даёт ***Catboosclassifier***. В случае картинок, видео и т. д. ***нейронные сети***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
